{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61276e4-df9d-4ab1-b3a4-84c043c94b48",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7236bbd3-d66a-4900-9015-3572f73e7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(__file__))))\n",
    "import os\n",
    "import torch \n",
    "from torch_geometric.data import Batch, Data\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scipy.stats import percentileofscore\n",
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "from argparse import Namespace\n",
    "from p_tqdm import p_map\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import partial\n",
    "from lightning_fabric.utilities.cloud_io import _load as pl_load\n",
    "import clipzyme.utils.loading as loaders\n",
    "from clipzyme.utils.loading import get_object\n",
    "from clipzyme.utils.smiles import remove_atom_maps\n",
    "from clipzyme.utils.pyg import from_mapped_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddcffd9-d357-4153-9020-7b8d55bed2c6",
   "metadata": {},
   "source": [
    "On Metrics:\n",
    "* bedroc85: top 3.5% contribute to 95% of the score (10K of 261,907 enzymes)\n",
    "* bedroc20: top 8% contribute to 80% of the score (21K of 261,907 enzymes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6eb681-ab2e-4d18-a1af-ea8e2f509373",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Collect screening set hidden representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e2f28-6983-41b5-9d33-960f84448325",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "```\n",
    "import torch \n",
    "import pickle\n",
    "from p_tqdm import p_map\n",
    "from functools import partial\n",
    "\n",
    "def read_protein(u, path):\n",
    "    try:\n",
    "        d = pickle.load(\n",
    "            open(f\"/home/results/metabolomics/clip/{path}/sample_{u}.hiddens\", 'rb')\n",
    "        )\n",
    "        return d['hidden']\n",
    "    except:\n",
    "        return\n",
    " \n",
    "screening_set= pickle.load(open(\"uniprot2sequence_standard_set_structs.p\", \"rb\"))\n",
    "screening_set = {k:v for k,v in screening_set.items() if v!= \"\"}\n",
    "# has structure and msa\n",
    "alphafold_files = pickle.load(open(\"alphafold_enzymes.p\", \"rb\"))\n",
    "msa_files = pickle.load(open(\"uniprot2msa_embedding.p\", \"rb\"))\n",
    "screening_set = {k:v for k,v in screening_set.items() if (k in alphafold_files) and (k in msa_files) and (len(v)<=650)}\n",
    "len(screening_set)\n",
    "screening_set_uniprots = list(screening_set.keys())\n",
    "\n",
    "experiment_name = \"4add9a242ca4f896cd31da4d0d129c63epoch=8\"\n",
    "\n",
    "read_protein_func = partial(read_protein, path = experiment_name)\n",
    "hiddens = p_map(read_protein_func, screening_set_uniprots)\n",
    "\n",
    "all_ec_uniprots_ = [u for u,h in zip(screening_set_uniprots, hiddens) if h is not None]\n",
    "hiddens = [h for h in hiddens if h is not None]\n",
    "hiddens = torch.stack(hiddens)\n",
    "\n",
    "pickle.dump({'hiddens': hiddens, 'uniprots': all_ec_uniprots_}, open(f\"precomputed_{experiment_name}.p\",'wb'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63edab4-27b7-4b15-9139-935b3af0a8f2",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dc9ba8e6-8084-47f7-aebf-3423af651629",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOND_TYPE = {\n",
    "    1: Chem.rdchem.BondType.SINGLE,\n",
    "    2: Chem.rdchem.BondType.DOUBLE,\n",
    "    3: Chem.rdchem.BondType.TRIPLE,\n",
    "    1.5: Chem.rdchem.BondType.AROMATIC,\n",
    "}\n",
    "\n",
    "clean_rxns_postsani = [\n",
    "    # two adjacent aromatic nitrogens should allow for H shift\n",
    "    Chem.AllChem.ReactionFromSmarts(\"[n;H1;+0:1]:[n;H0;+1:2]>>[n;H0;+0:1]:[n;H0;+0:2]\"),\n",
    "    # two aromatic nitrogens separated by one should allow for H shift\n",
    "    Chem.AllChem.ReactionFromSmarts(\n",
    "        \"[n;H1;+0:1]:[c:3]:[n;H0;+1:2]>>[n;H0;+0:1]:[*:3]:[n;H0;+0:2]\"\n",
    "    ),\n",
    "    Chem.AllChem.ReactionFromSmarts(\"[#7;H0;+:1]-[O;H1;+0:2]>>[#7;H0;+:1]-[O;H0;-:2]\"),\n",
    "    # neutralize C(=O)[O-]\n",
    "    Chem.AllChem.ReactionFromSmarts(\n",
    "        \"[C;H0;+0:1](=[O;H0;+0:2])[O;H0;-1:3]>>[C;H0;+0:1](=[O;H0;+0:2])[O;H1;+0:3]\"\n",
    "    ),\n",
    "    # turn neutral halogens into anions EXCEPT HCl\n",
    "    Chem.AllChem.ReactionFromSmarts(\"[I,Br,F;H1;D0;+0:1]>>[*;H0;-1:1]\"),\n",
    "    # inexplicable nitrogen anion in reactants gets fixed in prods\n",
    "    Chem.AllChem.ReactionFromSmarts(\"[N;H0;-1:1]([C:2])[C:3]>>[N;H1;+0:1]([*:2])[*:3]\"),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def robust_edit_mol(rmol, edits):\n",
    "    \"\"\"Simulate reaction via graph editing\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rmol : rdkit.Chem.rdchem.Mol\n",
    "        RDKit molecule instance for the reactants\n",
    "    bond_changes : list of 3-tuples\n",
    "        Each tuple is of form (atom1, atom2, change_type)\n",
    "    keep_atom_map : bool\n",
    "        Whether to keep atom mapping number. Default to False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred_smiles : list of str\n",
    "        SMILES for the edited molecule\n",
    "    \"\"\"\n",
    "\n",
    "    new_mol = Chem.RWMol(rmol)\n",
    "\n",
    "    # Keep track of aromatic nitrogens, might cause explicit hydrogen issues\n",
    "    aromatic_nitrogen_idx = set()\n",
    "    aromatic_carbonyl_adj_to_aromatic_nH = {}\n",
    "    aromatic_carbondeg3_adj_to_aromatic_nH0 = {}\n",
    "    for a in new_mol.GetAtoms():\n",
    "        if a.GetIsAromatic() and a.GetSymbol() == \"N\":\n",
    "            aromatic_nitrogen_idx.add(a.GetIdx())\n",
    "            for nbr in a.GetNeighbors():\n",
    "                if (\n",
    "                    a.GetNumExplicitHs() == 1\n",
    "                    and nbr.GetSymbol() == \"C\"\n",
    "                    and nbr.GetIsAromatic()\n",
    "                    and any(b.GetBondTypeAsDouble() == 2 for b in nbr.GetBonds())\n",
    "                ):\n",
    "                    aromatic_carbonyl_adj_to_aromatic_nH[nbr.GetIdx()] = a.GetIdx()\n",
    "                elif (\n",
    "                    a.GetNumExplicitHs() == 0\n",
    "                    and nbr.GetSymbol() == \"C\"\n",
    "                    and nbr.GetIsAromatic()\n",
    "                    and len(nbr.GetBonds()) == 3\n",
    "                ):\n",
    "                    aromatic_carbondeg3_adj_to_aromatic_nH0[nbr.GetIdx()] = a.GetIdx()\n",
    "        else:\n",
    "            a.SetNumExplicitHs(0)\n",
    "    new_mol.UpdatePropertyCache()\n",
    "\n",
    "    amap = {}\n",
    "    for atom in rmol.GetAtoms():\n",
    "        amap[atom.GetIntProp(\"molAtomMapNumber\")] = atom.GetIdx()  # new index to old index\n",
    "\n",
    "    # Apply the edits as predicted\n",
    "    for x, y, t in edits:\n",
    "        bond = new_mol.GetBondBetweenAtoms(amap[x], amap[y])\n",
    "        a1 = new_mol.GetAtomWithIdx(amap[x])\n",
    "        a2 = new_mol.GetAtomWithIdx(amap[y])\n",
    "        if bond is not None:\n",
    "            new_mol.RemoveBond(amap[x], amap[y])\n",
    "\n",
    "            # Are we losing a bond on an aromatic nitrogen?\n",
    "            if bond.GetBondTypeAsDouble() == 1.0:\n",
    "                if amap[x] in aromatic_nitrogen_idx:\n",
    "                    if a1.GetTotalNumHs() == 0:\n",
    "                        a1.SetNumExplicitHs(1)\n",
    "                    elif a1.GetFormalCharge() == 1:\n",
    "                        a1.SetFormalCharge(0)\n",
    "                elif amap[y] in aromatic_nitrogen_idx:\n",
    "                    if a2.GetTotalNumHs() == 0:\n",
    "                        a2.SetNumExplicitHs(1)\n",
    "                    elif a2.GetFormalCharge() == 1:\n",
    "                        a2.SetFormalCharge(0)\n",
    "\n",
    "            # Are we losing a c=O bond on an aromatic ring? If so, remove H from adjacent nH if appropriate\n",
    "            if bond.GetBondTypeAsDouble() == 2.0:\n",
    "                if amap[x] in aromatic_carbonyl_adj_to_aromatic_nH:\n",
    "                    new_mol.GetAtomWithIdx(\n",
    "                        aromatic_carbonyl_adj_to_aromatic_nH[amap[x]]\n",
    "                    ).SetNumExplicitHs(0)\n",
    "                elif amap[y] in aromatic_carbonyl_adj_to_aromatic_nH:\n",
    "                    new_mol.GetAtomWithIdx(\n",
    "                        aromatic_carbonyl_adj_to_aromatic_nH[amap[y]]\n",
    "                    ).SetNumExplicitHs(0)\n",
    "\n",
    "        if t > 0:\n",
    "            new_mol.AddBond(amap[x], amap[y], BOND_TYPE[t])\n",
    "\n",
    "            # Special alkylation case?\n",
    "            if t == 1:\n",
    "                if amap[x] in aromatic_nitrogen_idx:\n",
    "                    if a1.GetTotalNumHs() == 1:\n",
    "                        a1.SetNumExplicitHs(0)\n",
    "                    else:\n",
    "                        a1.SetFormalCharge(1)\n",
    "                elif amap[y] in aromatic_nitrogen_idx:\n",
    "                    if a2.GetTotalNumHs() == 1:\n",
    "                        a2.SetNumExplicitHs(0)\n",
    "                    else:\n",
    "                        a2.SetFormalCharge(1)\n",
    "\n",
    "            # Are we getting a c=O bond on an aromatic ring? If so, add H to adjacent nH0 if appropriate\n",
    "            if t == 2:\n",
    "                if amap[x] in aromatic_carbondeg3_adj_to_aromatic_nH0:\n",
    "                    new_mol.GetAtomWithIdx(\n",
    "                        aromatic_carbondeg3_adj_to_aromatic_nH0[amap[x]]\n",
    "                    ).SetNumExplicitHs(1)\n",
    "                elif amap[y] in aromatic_carbondeg3_adj_to_aromatic_nH0:\n",
    "                    new_mol.GetAtomWithIdx(\n",
    "                        aromatic_carbondeg3_adj_to_aromatic_nH0[amap[y]]\n",
    "                    ).SetNumExplicitHs(1)\n",
    "\n",
    "    pred_mol = new_mol.GetMol()\n",
    "\n",
    "    # Clear formal charges to make molecules valid\n",
    "    # Note: because S and P (among others) can change valence, be more flexible\n",
    "    for atom in pred_mol.GetAtoms():\n",
    "        # atom.ClearProp(\"molAtomMapNumber\")\n",
    "        if (\n",
    "            atom.GetSymbol() == \"N\" and atom.GetFormalCharge() == 1\n",
    "        ):  # exclude negatively-charged azide\n",
    "            bond_vals = sum([bond.GetBondTypeAsDouble() for bond in atom.GetBonds()])\n",
    "            if bond_vals <= 3:\n",
    "                atom.SetFormalCharge(0)\n",
    "        elif (\n",
    "            atom.GetSymbol() == \"N\" and atom.GetFormalCharge() == -1\n",
    "        ):  # handle negatively-charged azide addition\n",
    "            bond_vals = sum([bond.GetBondTypeAsDouble() for bond in atom.GetBonds()])\n",
    "            if bond_vals == 3 and any(\n",
    "                [nbr.GetSymbol() == \"N\" for nbr in atom.GetNeighbors()]\n",
    "            ):\n",
    "                atom.SetFormalCharge(0)\n",
    "        elif atom.GetSymbol() == \"N\":\n",
    "            bond_vals = sum([bond.GetBondTypeAsDouble() for bond in atom.GetBonds()])\n",
    "            if (\n",
    "                bond_vals == 4 and not atom.GetIsAromatic()\n",
    "            ):  # and atom.IsInRingSize(5)):\n",
    "                atom.SetFormalCharge(1)\n",
    "        elif atom.GetSymbol() == \"C\" and atom.GetFormalCharge() != 0:\n",
    "            atom.SetFormalCharge(0)\n",
    "        elif atom.GetSymbol() == \"O\" and atom.GetFormalCharge() != 0:\n",
    "            bond_vals = (\n",
    "                sum([bond.GetBondTypeAsDouble() for bond in atom.GetBonds()])\n",
    "                + atom.GetNumExplicitHs()\n",
    "            )\n",
    "            if bond_vals == 2:\n",
    "                atom.SetFormalCharge(0)\n",
    "        elif atom.GetSymbol() in [\"Cl\", \"Br\", \"I\", \"F\"] and atom.GetFormalCharge() != 0:\n",
    "            bond_vals = sum([bond.GetBondTypeAsDouble() for bond in atom.GetBonds()])\n",
    "            if bond_vals == 1:\n",
    "                atom.SetFormalCharge(0)\n",
    "        elif atom.GetSymbol() == \"S\" and atom.GetFormalCharge() != 0:\n",
    "            bond_vals = sum([bond.GetBondTypeAsDouble() for bond in atom.GetBonds()])\n",
    "            if bond_vals in [2, 4, 6]:\n",
    "                atom.SetFormalCharge(0)\n",
    "        elif (\n",
    "            atom.GetSymbol() == \"P\"\n",
    "        ):  # quartenary phosphorous should be pos. charge with 0 H\n",
    "            bond_vals = [bond.GetBondTypeAsDouble() for bond in atom.GetBonds()]\n",
    "            if sum(bond_vals) == 4 and len(bond_vals) == 4:\n",
    "                atom.SetFormalCharge(1)\n",
    "                atom.SetNumExplicitHs(0)\n",
    "            elif sum(bond_vals) == 3 and len(bond_vals) == 3:  # make sure neutral\n",
    "                atom.SetFormalCharge(0)\n",
    "        elif atom.GetSymbol() == \"B\":  # quartenary boron should be neg. charge with 0 H\n",
    "            bond_vals = [bond.GetBondTypeAsDouble() for bond in atom.GetBonds()]\n",
    "            if sum(bond_vals) == 4 and len(bond_vals) == 4:\n",
    "                atom.SetFormalCharge(-1)\n",
    "                atom.SetNumExplicitHs(0)\n",
    "        elif atom.GetSymbol() in [\"Mg\", \"Zn\"]:\n",
    "            bond_vals = [bond.GetBondTypeAsDouble() for bond in atom.GetBonds()]\n",
    "            if sum(bond_vals) == 1 and len(bond_vals) == 1:\n",
    "                atom.SetFormalCharge(1)\n",
    "        elif atom.GetSymbol() == \"Si\":\n",
    "            bond_vals = [bond.GetBondTypeAsDouble() for bond in atom.GetBonds()]\n",
    "            if sum(bond_vals) == len(bond_vals):\n",
    "                atom.SetNumExplicitHs(max(0, 4 - len(bond_vals)))\n",
    "\n",
    "    # Bounce to/from SMILES to try to sanitize\n",
    "    pred_smiles = Chem.MolToSmiles(pred_mol)\n",
    "    pred_list = pred_smiles.split(\".\")\n",
    "    pred_mols = [Chem.MolFromSmiles(pred_smiles) for pred_smiles in pred_list]\n",
    "\n",
    "    for i, mol in enumerate(pred_mols):\n",
    "        # Check if we failed/succeeded in previous step\n",
    "        if mol is None:\n",
    "            # print('##### Unparseable mol: {}'.format(pred_list[i]))\n",
    "            continue\n",
    "\n",
    "        # Else, try post-sanitiztion fixes in structure\n",
    "        mol = Chem.MolFromSmiles(Chem.MolToSmiles(mol))\n",
    "        if mol is None:\n",
    "            continue\n",
    "        for rxn in clean_rxns_postsani:\n",
    "            out = rxn.RunReactants((mol,))\n",
    "            if out:\n",
    "                try:\n",
    "                    Chem.SanitizeMol(out[0][0])\n",
    "                    pred_mols[i] = Chem.MolFromSmiles(Chem.MolToSmiles(out[0][0]))\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    # print(e)\n",
    "                    # print('Could not sanitize postsani reaction product: {}'.format(Chem.MolToSmiles(out[0][0])))\n",
    "                    # print('Original molecule was: {}'.format(Chem.MolToSmiles(mol)))\n",
    "    pred_smiles = [\n",
    "        Chem.MolToSmiles(pred_mol) for pred_mol in pred_mols if pred_mol is not None\n",
    "    ]\n",
    "\n",
    "    return pred_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0e772940-280a-487e-aa39-84eed09d49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics, percentiles, exclude_label=None):\n",
    "    if exclude_label:\n",
    "        b85 = [j for i,j in enumerate(metrics[\"bedroc85\"]) if (not exclude_label[i]) and (not np.isnan(j)) ]\n",
    "        b20 = [j for i,j in enumerate(metrics[\"bedroc20\"]) if (not exclude_label[i]) and (not np.isnan(j)) ]\n",
    "        ef1 = [j for i,j in enumerate(metrics[\"ef0.1\"]) if (not exclude_label[i]) and (not np.isnan(j)) ]\n",
    "        ef05 = [j for i,j in enumerate(metrics[\"ef0.05\"]) if (not exclude_label[i]) and (not np.isnan(j)) ]\n",
    "        percs = [j for i,j in enumerate(percentiles) if not exclude_label[i]]\n",
    "    else:\n",
    "        b85 = metrics[\"bedroc85\"]\n",
    "        b20 = metrics[\"bedroc20\"]\n",
    "        ef1 = metrics[\"ef0.1\"]\n",
    "        ef05 = metrics[\"ef0.05\"]\n",
    "        percs = percentiles\n",
    "        \n",
    "    print(\n",
    "        \"\"\"\n",
    "        * mean BEDROC_85:\\t {}\n",
    "        * mean BEDROC_20:\\t {}\n",
    "        * mean EF_0.05:\\t {}\n",
    "        * mean EF_0.1:\\t {}\n",
    "        * mean percentile:\\t {}\n",
    "        * median percentile:\\t {}\n",
    "        \"\"\".format(\n",
    "            np.mean(b85), \n",
    "            np.mean(b20),\n",
    "            np.mean(ef05), \n",
    "            np.mean(ef1),\n",
    "            np.mean(percs),\n",
    "            np.median(percs)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "584d04ec-b1ab-4997-bfd2-e0edbf4d98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bedroc_score(y_true, y_pred, decreasing=True, alpha=20.0):\n",
    "\n",
    "    \"\"\"BEDROC metric implemented according to Truchon and Bayley.\n",
    "\n",
    "    The Boltzmann Enhanced Descrimination of the Receiver Operator\n",
    "    Characteristic (BEDROC) score is a modification of the Receiver Operator\n",
    "    Characteristic (ROC) score that allows for a factor of *early recognition*.\n",
    "\n",
    "    References:\n",
    "        The original paper by Truchon et al. is located at `10.1021/ci600426e\n",
    "        <http://dx.doi.org/10.1021/ci600426e>`_.\n",
    "\n",
    "    Args:\n",
    "        y_true (array_like):\n",
    "            Binary class labels. 1 for positive class, 0 otherwise.\n",
    "        y_pred (array_like):\n",
    "            Prediction values.\n",
    "        decreasing (bool):\n",
    "            True if high values of ``y_pred`` correlates to positive class.\n",
    "        alpha (float):\n",
    "            Early recognition parameter.\n",
    "\n",
    "    Returns:\n",
    "        float:\n",
    "            Value in interval [0, 1] indicating degree to which the predictive\n",
    "            technique employed detects (early) the positive class.\n",
    "     \"\"\"\n",
    "\n",
    "    assert len(y_true) == len(y_pred), \\\n",
    "        'The number of scores must be equal to the number of labels'\n",
    "\n",
    "    big_n = len(y_true)\n",
    "    n = sum(y_true == 1)\n",
    "\n",
    "    if decreasing:\n",
    "        order = np.argsort(-y_pred)\n",
    "    else:\n",
    "        order = np.argsort(y_pred)\n",
    "\n",
    "    m_rank = (y_true[order] == 1).nonzero()[0]\n",
    "\n",
    "    s = np.sum(np.exp(-alpha * m_rank / big_n))\n",
    "\n",
    "    r_a = n / big_n\n",
    "\n",
    "    rand_sum = r_a * (1 - np.exp(-alpha))/(np.exp(alpha/big_n) - 1)\n",
    "\n",
    "    fac = r_a * np.sinh(alpha / 2) / (np.cosh(alpha / 2) -\n",
    "                                      np.cosh(alpha/2 - alpha * r_a))\n",
    "\n",
    "    cte = 1 / (1 - np.exp(alpha * (1 - r_a)))\n",
    "\n",
    "    return s * fac / rand_sum + cte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5a84938c-7c6c-423a-b47a-cacf19a8de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrichment_score(y_true, y_pred, decreasing=True, chi=0.1):\n",
    "    \"\"\"Enrichment Factor metric\n",
    "\n",
    "        How many more actives we find within a defined \n",
    "        “early recognition” fraction of the ordered \n",
    "        list relative to a random distribution\n",
    "\n",
    "\n",
    "              n_actives_in_sampled_set / n_sampled_set      n_actives_in_sampled_set\n",
    "        EF =  ----------------------------------------  = ----------------------------\n",
    "                   total_actives / total_ligands               chi * total_actives\n",
    "\n",
    "    Args:\n",
    "        y_true (array_like):\n",
    "            Binary class labels. 1 for positive class, 0 otherwise.\n",
    "        y_pred (array_like):\n",
    "            Prediction values.\n",
    "        decreasing (bool):\n",
    "              True if high values of ``y_pred`` correlates to positive class.\n",
    "      \n",
    "    Returns:\n",
    "        float:\n",
    "            Value in interval [0, tau]\n",
    "                tau = 1/chi if chi >= n/N\n",
    "                tau = N/n if chi < n/N\n",
    "     \"\"\"\n",
    "    big_n = len(y_true)\n",
    "    n = sum(y_true == 1)\n",
    "    \n",
    "    if decreasing:\n",
    "        order = np.argsort(-y_pred)\n",
    "    else:\n",
    "        order = np.argsort(y_pred)\n",
    "\n",
    "    k = math.floor(chi * big_n)\n",
    "    \n",
    "    num_in_topk = (y_true[order] == 1)[:k].sum()\n",
    "\n",
    "    ef = (num_in_topk) / (chi * n)\n",
    "        \n",
    "    return ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9c0ab2b6-20cf-47c1-a746-1c3b0b3e32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clean_prediction(path):\n",
    "    u2ec = {}\n",
    "    result = open(path, 'r')\n",
    "    csvreader = csv.reader(result, delimiter=',')\n",
    "    pred_label = []\n",
    "    for row in csvreader:\n",
    "        preds_ec_lst = set()\n",
    "        uni = row[0]\n",
    "        preds_with_dist = row[1:]\n",
    "        for pred_ec_dist in preds_with_dist:\n",
    "            # get EC number 3.5.2.6 from EC:3.5.2.6/10.8359\n",
    "            ec_i = pred_ec_dist.split(\":\")[1].split(\"/\")[0]\n",
    "            preds_ec_lst.add(ec_i)\n",
    "        u2ec[uni] = preds_ec_lst\n",
    "    return u2ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2f2fdec6-9153-4b23-843d-2257f68267d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_protein(u, path):\n",
    "    try:\n",
    "        d = pickle.load(\n",
    "            open(f\"/home/results/metabolomics/clip/{path}/sample_{u}.hiddens\", 'rb')\n",
    "        )\n",
    "        return d['hidden']\n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6369161e-71c2-4b67-84ef-c6280acdaf5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Screening Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c01211-6ff8-424b-ac09-320939930094",
   "metadata": {},
   "outputs": [],
   "source": [
    "screening_set= pickle.load(open(\"/home/uniprot2sequence_standard_set_structs.p\", \"rb\"))\n",
    "print(f\"Original screening set size: {len(screening_set)}\")\n",
    "\n",
    "# remove empty\n",
    "screening_set = {k:v for k,v in screening_set.items() if v!= \"\"}\n",
    "\n",
    "# has structure and msa\n",
    "alphafold_files = pickle.load(open(\"/home/alphafold_enzymes.p\", \"rb\"))\n",
    "msa_files = pickle.load(open(\"/home/uniprot2msa_embedding.p\", \"rb\"))\n",
    "\n",
    "screening_set = {k:v for k,v in screening_set.items() if (k in alphafold_files) and (k in msa_files) and (len(v)<=650)}\n",
    "print(f\"Final screening set size: {len(screening_set)}\")\n",
    "# as list\n",
    "screening_set_uniprots = list(screening_set.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b665c-4907-40dc-8de3-743248a23050",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b4188-2109-42f5-8bc7-64c5fdd9d832",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5c50d84b-6ee1-4bb1-b8f3-0528d8e18fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint and args\n",
    "args_path = '/home/logs/metabo/bf6b607124c5cca3430fc0c2ee1148dd.args'\n",
    "args = Namespace(**pickle.load(open(args_path,'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "343e4ab6-aa0f-4f2f-8543-6585519cba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.cache_path= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1e248-d8c0-4dd7-8bc1-51c65efa4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_object(args.dataset_name, 'dataset')(args, 'train')\n",
    "val_dataset = get_object(args.dataset_name, 'dataset')(args, 'dev')\n",
    "test_dataset = get_object(args.dataset_name, 'dataset')(args, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2b48baad-b854-4321-8d33-fe87dbd9aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uniprots = set(d['uniprot_id'] for d in  train_dataset.dataset)\n",
    "train_reactions = set(d['reaction_string'] for d in  train_dataset.dataset)\n",
    "train_ecs = set(d['ec'] for d in  train_dataset.dataset)\n",
    "\n",
    "train_ecs1 = set(d['ec1'] for d in  train_dataset.dataset)\n",
    "train_ecs2 = set(d['ec2'] for d in  train_dataset.dataset)\n",
    "train_ecs3 = set(d['ec3'] for d in  train_dataset.dataset)\n",
    "train_ecs4 = set(d['ec4'] for d in  train_dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f7d40e2e-85bb-4f3a-8859-c8318b5abc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rules = set([d['rule_id'] for d in train_dataset.dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c1dda803-ff7b-4020-a068-533048842575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset\n",
    "full_dataset = json.load(open(args.dataset_file_path, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "26c19c42-8e66-41fb-ae34-29942c2aadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction2unis = defaultdict(set)\n",
    "for d in full_dataset:\n",
    "    if d['protein_db'] not in [\"swissprot\", \"uniprot\"]:\n",
    "        continue\n",
    "\n",
    "    # without maps\n",
    "    r = '.'.join(sorted(d['reactants'])) + '>>' + '.'.join(sorted(d['products']))\n",
    "    for p in eval(d['protein_refs']):\n",
    "        reaction2unis[r].add(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3496f-6669-4b78-8999-446f7b072f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reaction2unis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fa22ac3e-5eaf-4ed6-80c3-96d60d17899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_dataset = pickle.load(open(args.dataset_cache_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1cf17-e62d-4a14-a8c7-a85930f5635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([d['ec'][0] for d in cached_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f41d53-1ef6-4074-a09a-b7d0d2d21f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([d['rule_id'] for d in cached_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6339e42-d579-4726-ac4c-0c4f5f66d530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb2fb3f-4917-4966-8b02-c28efb749e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset.uniprot2sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaec200-4e29-4ba4-ab21-ddedf6efbff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentileofscore([len(v) for k,v in train_dataset.uniprot2sequence.items() if v is not None], 650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1d371-bc94-4eec-a8a3-fd8a3ddc7543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f53b1e09-f381-49f3-8184-0c22d10b1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "ec2uniprots = pickle.load(open('/home/Brenda/ec2uniprot_2023_1.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "55cccf5b-30b2-4e1f-90b8-9f0aece68c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2uniprots_levels = {}\n",
    "for level in [1, 2, 3, 4]:\n",
    "    s2ecs_ = defaultdict(set)\n",
    "    for ec, us in ec2uniprots.items():\n",
    "        e_ = '.'.join(ec.split('.')[:level])\n",
    "        s2ecs_[e_].update( us )\n",
    "    ec2uniprots_levels[level] = s2ecs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e79bc-a42b-471c-a456-22bdd146fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in ec2uniprots_levels[1].items():\n",
    "    print(f\"EC {k}: {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92419fd0-3aaf-4109-9500-bad667daadda",
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in [1, 2, 3, 4]:\n",
    "    avgsize = np.mean([len(v) for v in ec2uniprots_levels[level].values() ])\n",
    "    print(f\"Level {level}: {avgsize}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3d5da-b6e0-4c9f-954e-83a559facf7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77c4e250-50ae-4b5b-9c57-32fd603574f4",
   "metadata": {},
   "source": [
    "## load model and screening set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a205f99b-e85d-468f-b8d4-ad00610347ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/snapshots/metabolomics/bf6b607124c5cca3430fc0c2ee1148dd/bf6b607124c5cca3430fc0c2ee1148ddepoch=26.ckpt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "args = checkpoint['hyper_parameters']['args']\n",
    "args.from_checkpoint = True\n",
    "args.checkpoint_path = checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3aac6c57-ac88-447a-b3d3-cfbca81fabc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckptpath = Path(args.checkpoint_path)\n",
    "screenset = pickle.load(open(f\"precomputed_{ckptpath.stem}.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "557ddd06-3185-424e-a02c-ca6999d044b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "91a62d66-b7de-474b-8cf4-0d5d11910035",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.do_ec_task = False\n",
    "args.train_esm_dir = '/home/snapshots/metabolomics/esm2/checkpoints/esm2_t30_150M_UR50D.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e9a1b-ca53-470c-a2ef-66a9b2481e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = loaders.get_lightning_model(args)\n",
    "except:\n",
    "    model = get_object(args.lightning_name, \"lightning\")(args)\n",
    "    checkpoint = pl_load(args.checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model = model.to(DEVICE)\n",
    "print(model.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6a3059f0-0632-4b5f-ab1e-ebf8baa12d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_hiddens = screenset[\"hiddens\"]\n",
    "screen_unis = screenset[\"uniprots\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45bf65-00d8-4973-86ac-117fa3f7062b",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d3e06-280d-4524-b994-1ba33856dddd",
   "metadata": {},
   "source": [
    "### table 1: ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0afc6759-991d-47da-9265-055005337e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_unis_set = set(screen_unis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "be0e7b52-f3f7-4128-b878-9f558d2796cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.args.use_as_mol_encoder = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "98131cef-816e-43b1-850d-ee4a92252c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.args.ec_levels_one_hot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26129692-35fe-414b-b210-e34bbcc6f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(screen_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7385a0e-228b-4f94-85f6-26e35066730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = []\n",
    "metrics = defaultdict(list)\n",
    "data_samples = []\n",
    "data_sample_ids = []\n",
    "all_substrate_features = []\n",
    "reaction_set = set()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ref_id in tqdm(range(len(test_dataset)), ncols=100, total=len(test_dataset)):\n",
    "    \n",
    "        rxn_uni = test_dataset.dataset[ref_id]['reaction_string']\n",
    "        if rxn_uni in train_reactions:\n",
    "            continue \n",
    "            \n",
    "        if rxn_uni in reaction_set:\n",
    "            continue \n",
    "    \n",
    "        reaction_set.add(rxn_uni)\n",
    "        \n",
    "        data_samples.append( test_dataset.dataset[ref_id] )\n",
    "        data_sample_ids.append(ref_id)\n",
    "        \n",
    "        ref_sample = loaders.default_collate([test_dataset[ref_id]])\n",
    "        for key in ['reactants', 'products', 'graph', 'mol']:\n",
    "            if key in ref_sample:\n",
    "                ref_sample[key] = ref_sample[key].to(DEVICE)\n",
    "        out = model.model(ref_sample)\n",
    "        substrate_features = out['hidden']\n",
    "        substrate_features = substrate_features.cpu()\n",
    "        all_substrate_features.append(substrate_features)\n",
    "\n",
    "        ref_unis = reaction2unis[ test_dataset.dataset[ref_id]['reaction_string']  ]\n",
    "\n",
    "        scores = screen_hiddens @ substrate_features.T\n",
    "        scores = scores.squeeze().tolist()\n",
    "        \n",
    "        labels = [ int(u in reaction2unis[ test_dataset.dataset[ref_id]['reaction_string'] ]) for u in screen_unis]\n",
    "        \n",
    "        # compute metrics\n",
    "        labels = np.array(labels)\n",
    "        scores = np.array(scores)\n",
    "        metrics[\"bedroc85\"].append( bedroc_score(labels, scores, decreasing=True, alpha=85.0) )\n",
    "        metrics[\"bedroc20\"].append( bedroc_score(labels, scores, decreasing=True, alpha=20.0) )\n",
    "        metrics[\"ef0.1\"].append( enrichment_score(labels, scores, decreasing=True, chi=0.1) )\n",
    "        metrics[\"ef0.05\"].append( enrichment_score(labels, scores, decreasing=True, chi=0.05) )\n",
    "        \n",
    "\n",
    "        # percentile\n",
    "        refscore = max(s for s,l in zip(scores,labels) if l)\n",
    "        percentiles.append( percentileofscore(scores, refscore) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451f4a9-930c-4aa1-a1a8-4f15e25f3221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cc150fa1-6784-4ca4-aa5e-807926d44b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_substrate_features = torch.concat(all_substrate_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c4d00-37a1-4166-a675-2db1b4fe4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_substrate_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e1c7489a-0418-4204-a2c1-ced9ee148904",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = all_substrate_features @ screen_hiddens.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9094e642-823f-47af-9600-6db054050c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [ (d['uniprot_id'] in train_uniprots) for d in data_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3fb818-3205-4714-9474-44bb0decec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(metrics, percentiles, exclude_label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67269562-fa6d-4d94-aff5-2591ce996125",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### table 5: within EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2c822fd3-b2f2-4012-ad93-22cedeff83f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2uniprot = {k:set(v) for k,v in train_dataset.ec2uniprot.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b1115e32-85dd-4e2d-9efd-56130097fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2uniprot_levels = {k: defaultdict(set) for k in [1,2,3,4]}\n",
    "for level in [1,2,3,4]:\n",
    "    for ec_, u_ in ec2uniprot.items():\n",
    "        ec2uniprot_levels[level]['.'.join(ec_.split('.')[:level])].update(u_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50ea58-c35d-4d78-b6c0-68335b14428c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740675a5-abd6-4cdf-ab67-193ae393957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how well can rank uniprots in an EC for a given reaction\n",
    "\n",
    "data_samples_ec = []\n",
    "percentiles = []\n",
    "metrics = defaultdict(list)\n",
    "reaction_set = set()\n",
    "\n",
    "for level in [1]\n",
    "    percentiles = []\n",
    "    data_samples_ec = []\n",
    "    # metrics = defaultdict(list)\n",
    "    reaction_set = set()\n",
    "    for idx, ref_sample in tqdm(enumerate(data_samples), ncols=100, total=len(data_samples)):\n",
    "    \n",
    "        if ref_sample['reaction_string'] in reaction_set:\n",
    "            continue\n",
    "        \n",
    "        if  train_dataset.ec2uniprot.get( ref_sample['ec'], None) is None:\n",
    "            continue\n",
    "    \n",
    "        # take only the sequences in EC\n",
    "        ec_ = '.'.join(ref_sample['ec'].split('.')[:level])\n",
    "        ec_unis = [u for u in screen_unis if u in ec2uniprot_levels[level][ec_] ]\n",
    "        # label scores\n",
    "        labels = [ int(u in reaction2unis[ ref_sample['reaction_string'] ]) for u in ec_unis]\n",
    "        if not any(labels):\n",
    "            continue \n",
    "    \n",
    "        reaction_set.add(ref_sample['reaction_string'])\n",
    "        data_samples_ec.append(ref_sample)\n",
    "        \n",
    "        scores = sim_matrix[idx].squeeze().numpy()  \n",
    "    \n",
    "        scores = [s for s, u in zip(scores, screen_unis) if u in ec2uniprot_levels[level][ec_]  ]\n",
    "    \n",
    "        ref_score = max( scores[i] for i,l in enumerate(labels) if l)\n",
    "        \n",
    "        metrics[\"bedroc85\"].append( bedroc_score(np.array(labels), np.array(scores), decreasing=True, alpha=85.0) )\n",
    "        metrics[\"bedroc20\"].append( bedroc_score(np.array(labels), np.array(scores), decreasing=True, alpha=20.0) )\n",
    "        metrics[\"ef0.1\"].append( enrichment_score(np.array(labels), np.array(scores), decreasing=True, chi=0.1) )\n",
    "        metrics[\"ef0.05\"].append( enrichment_score(np.array(labels), np.array(scores), decreasing=True, chi=0.05) )\n",
    "        \n",
    "        percentiles.append( \n",
    "            percentileofscore(scores, ref_score ) \n",
    "        )\n",
    "    print(f\"Level: {level}\")\n",
    "    print_metrics(metrics, percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae2104-18aa-483f-b189-84b4a7baf364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3f60403-0d64-4e9a-87de-3dad27e2e640",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### table 2a: EC-based screening using distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a4799ec7-4313-41b5-8831-f82a557f19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ec_id_dict(csv_name: str) -> dict:\n",
    "    csv_file = open(csv_name)\n",
    "    csvreader = csv.reader(csv_file, delimiter=\"\\t\")\n",
    "    id_ec = {}\n",
    "    ec_id = {}\n",
    "\n",
    "    for i, rows in enumerate(csvreader):\n",
    "        if i > 0:\n",
    "            id_ec[rows[0]] = rows[1].split(\";\")\n",
    "            for ec in rows[1].split(\";\"):\n",
    "                if ec not in ec_id.keys():\n",
    "                    ec_id[ec] = set()\n",
    "                    ec_id[ec].add(rows[0])\n",
    "                else:\n",
    "                    ec_id[ec].add(rows[0])\n",
    "    return id_ec, ec_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f69080d7-af29-4a96-8e8f-e686ed3ec025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_center(model_emb, ec_id_dict):\n",
    "    cluster_center_model = {}\n",
    "    id_counter = 0\n",
    "    with torch.no_grad():\n",
    "        for ec in tqdm(list(ec_id_dict.keys())):\n",
    "            ids_for_query = list(ec_id_dict[ec])\n",
    "            id_counter_prime = id_counter + len(ids_for_query)\n",
    "            emb_cluster = model_emb[id_counter:id_counter_prime]\n",
    "            cluster_center = emb_cluster.mean(dim=0)\n",
    "            cluster_center_model[ec] = cluster_center.detach().cpu()\n",
    "            id_counter = id_counter_prime\n",
    "    return cluster_center_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753549b6-d139-4ab7-9192-addf9c658076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c43d172b-b3ab-47bf-a155-a3cea287c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN embeddings for screening set\n",
    "clean_screening_set_embed = torch.load('CLEAN_screening_set_embed.p', map_location='cpu')\n",
    "\n",
    "# reference sequences for computing EC anchors / clusters\n",
    "clean_split100 = pickle.load(open('CLEAN_split100_embed.p', 'rb'))\n",
    "clean_split100_uni2embed = {u:h for u,h in zip(clean_split100['uniprots'], clean_split100['clean_embeddings'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cadb2e5f-a4c3-4a5b-8340-c3696d19cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# row identifiers\n",
    "row_uniprots = pd.read_csv(\"uniprot2sequence_standard_set_structs.csv\", delimiter='\\t')\n",
    "row_uniprots = list(row_uniprots['Entry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "dbeba79b-e324-4231-b087-cac49294923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order screen embeds in same order as screen_unis\n",
    "clean_screening_uni_2_embed = { u: h for u,h in zip(row_uniprots, clean_screening_set_embed) }\n",
    "clean_screening_set_embed_ = [clean_screening_uni_2_embed[u] for u in screen_unis]\n",
    "clean_screening_set_embed = torch.vstack(clean_screening_set_embed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c0f23-1ec3-42f0-839d-f18ba7c9813c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "89e9d25d-b48c-499e-9867-b51f08b62e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster centers\n",
    "id_ec_train, ec_id_dict_train = get_ec_id_dict(\"CLEAN_split100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "228f0dac-b0ea-4c4d-90bd-36b1dfb580ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ecs = list(ec_id_dict_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6398b6dd-9aa2-407a-88f3-99c64ca53de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mapping of EC -> UniProts for each level\n",
    "clean_ec2unis_level = {}\n",
    "for level in [1,2,3,4]:\n",
    "    clean_ec2unis_level[level] = defaultdict(set)\n",
    "    for k,v in ec_id_dict_train.items():\n",
    "        # get ec level\n",
    "        ec_l = '.'.join(k.split('.')[:level])\n",
    "        clean_ec2unis_level[level][ec_l].update(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b84ba-3f1e-4252-8ef7-dcd156fb7f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4083e651-9279-4907-a3e8-166d829604fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each level, compute distance matrix to the anchors\n",
    "\n",
    "eval_dist_levels = {}\n",
    "for level in [1,2,3,4]:\n",
    "    \n",
    "    # get cluster anchors\n",
    "    model_lookup_ec_keys, model_lookup = [], []\n",
    "    for k, uni_in_ec in clean_ec2unis_level[level].items():\n",
    "        embeds_ = torch.vstack([clean_split100_uni2embed[u] for u in uni_in_ec])\n",
    "        model_lookup.append(embeds_.mean(0))\n",
    "        model_lookup_ec_keys.append(k)\n",
    "    \n",
    "    model_lookup = torch.vstack(model_lookup)\n",
    "    \n",
    "    # distance: columns are ECs\n",
    "    eval_dist = torch.cdist(clean_screening_set_embed, model_lookup).detach()\n",
    "    eval_dist_levels[level] = (eval_dist, model_lookup_ec_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76296f9d-db5f-493e-bb62-519c6936d22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37a6f8-81fd-442a-ac68-881b55f72702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fae50-0dfb-4016-ab85-95ead7aa8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STRATEGY\n",
    "\n",
    "Given a reaction with EC 1.1.1.x: \n",
    "    - If anchor 1.1.1 exists:\n",
    "        Take distance of screening enzymes to that EC and use to rank\n",
    "    - Else:\n",
    "        Take distances to 1.1 (or higher until EC is found -- guaranteed to find highest level)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "level = 4\n",
    "\n",
    "reaction_set = set()\n",
    "percentiles = []\n",
    "metrics = defaultdict(list)\n",
    "\n",
    "for ref_id in tqdm(range(len(test_dataset)), ncols=100, total=len(test_dataset)):\n",
    "\n",
    "    # get sample\n",
    "    ref_sample = test_dataset.dataset[ref_id]\n",
    "\n",
    "    # check if reaction was in train set\n",
    "    rxn_uni = ref_sample['reaction_string']\n",
    "    if rxn_uni in train_reactions:\n",
    "        continue \n",
    "\n",
    "    # check if reaction was already evaluated in this loop\n",
    "    if rxn_uni in reaction_set:\n",
    "        continue \n",
    "\n",
    "    reaction_set.add(rxn_uni)\n",
    "\n",
    "    # get ec, level\n",
    "    ec = ref_sample[\"ec\"]\n",
    "\n",
    "    # if EC is not within split100 dataset, then check above \n",
    "    for prev_level in range(level, 0, -1):\n",
    "        ec_ = '.'.join(ec.split('.')[:prev_level])\n",
    "        level_eval_dist, level_model_lookup_ec_keys = eval_dist_levels[prev_level]\n",
    "        if ec_ in level_model_lookup_ec_keys:\n",
    "            break\n",
    "                          \n",
    "    lookup_col_idx = level_model_lookup_ec_keys.index(ec_)\n",
    "    scores = level_eval_dist[:,lookup_col_idx].numpy()\n",
    "    labels = np.array([ int(u in reaction2unis[rxn_uni]) for u in screen_unis])\n",
    "    \n",
    "    # # compute BEDROC        \n",
    "    metrics[\"bedroc85\"].append( bedroc_score(labels, scores, decreasing=False, alpha=85.0) )\n",
    "    metrics[\"bedroc20\"].append( bedroc_score(labels, scores, decreasing=False, alpha=20.0) )\n",
    "    metrics[\"ef0.1\"].append( enrichment_score(labels, scores, decreasing=False, chi=0.1) )\n",
    "    metrics[\"ef0.05\"].append( enrichment_score(labels, scores, decreasing=False, chi=0.05) )\n",
    "\n",
    "    # # percentile\n",
    "    # refscore = max(s for s,l in zip(scores,labels) if l)\n",
    "    # percentiles.append( percentileofscore(scores, refscore) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43732b1-cc7b-44fc-b159-92f84b8af3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LEVEL: {level}\")\n",
    "print_metrics(metrics, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e86107-22e0-47e6-9cc4-dbbfea3c0ea0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### table 2b: re-ranked EC with CLIPZyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9c185f5c-1627-4ad0-9966-d4453611f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_prediction = read_clean_prediction('CLEAN_uniprot2sequence_standard_set_structs_maxsep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "48b69888-5366-4448-b495-98d97e3f8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_unis_set = set(screen_unis)\n",
    "clean_ec2uniprots = {i: defaultdict(set) for i in [1,2,3,4]}\n",
    "for u, ecs in clean_prediction.items():\n",
    "    if u not in screen_unis_set:\n",
    "        continue\n",
    "    for ec in ecs:\n",
    "        for level in [1,2,3,4]:\n",
    "            ec_lvl = '.'.join(ec.split('.')[:level])\n",
    "            clean_ec2uniprots[level][ec_lvl].add(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "21d84ff4-375d-40b6-baaf-7397fd02db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_unis_array = np.array(screen_unis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "55b0fbdf-c238-41dc-9b0e-19c728b5626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = sim_matrix.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7512be-8f05-4a10-98d1-f9ac4c56bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STRATEGY:\n",
    "\n",
    "Use EC prediction to create ranked list of enzymes:\n",
    "\n",
    "Given a reaction with EC 1.1.1.x, create a list of enzymes according to:\n",
    "\n",
    "    - [ [proteins in 1.1.1] + [proteins in 1.1] + [proteins in 1] + [other] ]\n",
    "    - re-rank with CLIPZyme each susbet\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "level = 4\n",
    "\n",
    "percentiles = []\n",
    "metrics = defaultdict(list)\n",
    "reaction_set = set()\n",
    "\n",
    "for ref_id, ref_sample in tqdm(enumerate(data_samples), ncols=100, total=len(data_samples)):\n",
    "\n",
    "    # check if reaction was in train set\n",
    "    rxn_uni = ref_sample['reaction_string']\n",
    "    if rxn_uni in train_reactions:\n",
    "        continue \n",
    "\n",
    "    # check if reaction was already evaluated in this loop\n",
    "    if rxn_uni in reaction_set:\n",
    "        continue \n",
    "\n",
    "    reaction_set.add(rxn_uni)\n",
    "    \n",
    "    screening_scores = sim_matrix[ref_id] # rxn by enzymes\n",
    "\n",
    "    # sort once at the beginning\n",
    "    sorted_indices = np.argsort(-screening_scores) # high score should have low index to placed at the beginning of the list\n",
    "    sorted_screen_unis = screen_unis_array[sorted_indices]\n",
    "\n",
    "    \n",
    "    # put predictions together in ordered list and rank with CLIPZyme at each step\n",
    "    # this will determine final rank\n",
    "    predictions_uniprot = []\n",
    "    predictions_sofar = set()\n",
    "    for lvl in range(level, 0, -1):\n",
    "        # get ec, level\n",
    "        ec = '.'.join( ref_sample[\"ec\"].split('.')[:lvl] )\n",
    "        \n",
    "        # get uniprots annotated for ec, exclude those already added to list\n",
    "        ec_proteins = clean_ec2uniprots[lvl].get(ec, set()) - predictions_sofar\n",
    "        predictions_sofar.update(ec_proteins)\n",
    "        \n",
    "        # rank\n",
    "        if len(ec_proteins):\n",
    "            ec_proteins_list = list(ec_proteins)\n",
    "            proteins_select_mask = np.isin(sorted_screen_unis, ec_proteins_list) \n",
    "            proteins_select = sorted_screen_unis[proteins_select_mask]\n",
    "            predictions_uniprot.extend(proteins_select)\n",
    "\n",
    "    \n",
    "    # rank the rest\n",
    "    remaining_proteins = list(screen_unis_set - predictions_sofar)\n",
    "\n",
    "    proteins_select_mask = np.isin(sorted_screen_unis, remaining_proteins)\n",
    "    proteins_select = sorted_screen_unis[proteins_select_mask]\n",
    "    predictions_uniprot.extend(proteins_select)\n",
    "            \n",
    "    labels =  np.isin(predictions_uniprot, list(reaction2unis[rxn_uni]))\n",
    "    ranks = np.arange(len(labels))[::-1]\n",
    "\n",
    "    \n",
    "    # # # compute BEDROC, EF\n",
    "    metrics[\"bedroc85\"].append( bedroc_score(labels, ranks, decreasing=True, alpha=85.0) )\n",
    "    metrics[\"bedroc20\"].append( bedroc_score(labels, ranks, decreasing=True, alpha=20.0) )\n",
    "    metrics[\"ef0.1\"].append( enrichment_score(labels, ranks, decreasing=True, chi=0.1) )\n",
    "    metrics[\"ef0.05\"].append( enrichment_score(labels, ranks, decreasing=True, chi=0.05) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d0af1-57c5-49ec-912a-dce7b3bc8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Level: {level}\")\n",
    "print_metrics(metrics, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cc230-a96f-42bd-9d9e-b67ec82537b2",
   "metadata": {},
   "source": [
    "### table 8: re-ranked EC with CLEAN distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8201a-4a10-4a05-8f05-bd87d262eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"STRATEGY:\n",
    "\n",
    "Use EC prediction to create ranked list of enzymes:\n",
    "\n",
    "Given a reaction with EC 1.1.1.x, create a list of enzymes according to:\n",
    "\n",
    "    - [ [proteins in 1.1.1] + [proteins in 1.1] + [proteins in 1] + [other] ]\n",
    "    - re-rank with EC distances within each susbet\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "level = 1\n",
    "\n",
    "percentiles = []\n",
    "metrics = defaultdict(list)\n",
    "reaction_set = set()\n",
    "\n",
    "for ref_id, ref_sample in tqdm(enumerate(data_samples), ncols=100, total=len(data_samples)):\n",
    "\n",
    "    # check if reaction was in train set\n",
    "    rxn_uni = ref_sample['reaction_string']\n",
    "    if rxn_uni in train_reactions:\n",
    "        continue \n",
    "\n",
    "    # check if reaction was already evaluated in this loop\n",
    "    if rxn_uni in reaction_set:\n",
    "        continue \n",
    "\n",
    "    reaction_set.add(rxn_uni)\n",
    "\n",
    "    # distance most specific ec is used:\n",
    "    # if rxn ec = 1.1.1.1 and it exists in CLEAN anchor then use distances to it\n",
    "    # if it doesn't exist, try 1.1.1 and use distances to it\n",
    "    ec_found = False\n",
    "    for lvl in range(level, 0, -1):\n",
    "        ec = '.'.join( ref_sample[\"ec\"].split('.')[:lvl] )\n",
    "        level_eval_dist, level_model_lookup_ec_keys = eval_dist_levels[lvl]\n",
    "        if ec in level_model_lookup_ec_keys:\n",
    "            ec_found = True\n",
    "            lookup_col_idx = level_model_lookup_ec_keys.index(ec)\n",
    "            screening_scores = level_eval_dist[:,lookup_col_idx].numpy()\n",
    "        if ec_found:\n",
    "            break\n",
    "\n",
    "    \n",
    "    # sort once at the beginning\n",
    "    sorted_indices = np.argsort(screening_scores) # low score (distance) should have low index to placed at the beginning of the list\n",
    "    sorted_screen_unis = screen_unis_array[sorted_indices]\n",
    "\n",
    "    \n",
    "    # put predictions together in ordered list and rank with CLIPZyme at each step\n",
    "    # this will determine final rank\n",
    "    predictions_uniprot = []\n",
    "    predictions_sofar = set()\n",
    "    for lvl in range(level, 0, -1):\n",
    "        # get ec, level\n",
    "        ec = '.'.join( ref_sample[\"ec\"].split('.')[:lvl] )\n",
    "        \n",
    "        # get uniprots annotated for ec, exclude those already added to list\n",
    "        ec_proteins = clean_ec2uniprots[lvl].get(ec, set()) - predictions_sofar\n",
    "        predictions_sofar.update(ec_proteins)\n",
    "        \n",
    "        # rank\n",
    "        if len(ec_proteins):\n",
    "            ec_proteins_list = list(ec_proteins)\n",
    "            proteins_select_mask = np.isin(sorted_screen_unis, ec_proteins_list) \n",
    "            proteins_select = sorted_screen_unis[proteins_select_mask]\n",
    "            predictions_uniprot.extend(proteins_select)\n",
    "\n",
    "    \n",
    "    # rank the rest\n",
    "    remaining_proteins = list(screen_unis_set - predictions_sofar)\n",
    "\n",
    "    proteins_select_mask = np.isin(sorted_screen_unis, remaining_proteins)\n",
    "    proteins_select = sorted_screen_unis[proteins_select_mask]\n",
    "    predictions_uniprot.extend(proteins_select)\n",
    "            \n",
    "    labels =  np.isin(predictions_uniprot, list(reaction2unis[rxn_uni]))\n",
    "    ranks = np.arange(len(labels))[::-1]\n",
    "\n",
    "    \n",
    "    # # # compute BEDROC, EF\n",
    "    metrics[\"bedroc85\"].append( bedroc_score(labels, ranks, decreasing=True, alpha=85.0) )\n",
    "    metrics[\"bedroc20\"].append( bedroc_score(labels, ranks, decreasing=True, alpha=20.0) )\n",
    "    metrics[\"ef0.1\"].append( enrichment_score(labels, ranks, decreasing=True, chi=0.1) )\n",
    "    metrics[\"ef0.05\"].append( enrichment_score(labels, ranks, decreasing=True, chi=0.05) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bea42-2857-4d5b-849c-45d0fbc0dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Level: {level}\")\n",
    "print_metrics(metrics, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b1026-10fa-4da4-a492-65d0bc399269",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### table 3: other reactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4175820-1604-4bfd-b59e-1526ba60dd45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### unannotated enzymemap reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "bc353fa5-3933-479f-ae2a-3b8f52928e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unannotated_reaction2ec = defaultdict(set)\n",
    "unannotated_dataset = []\n",
    "for d in full_dataset:\n",
    "    if not isinstance(d['protein_db'], str):\n",
    "        assert np.isnan(d['protein_db'])\n",
    "        # without map\n",
    "        r = '.'.join(sorted(d['reactants'])) + '>>' + '.'.join(sorted(d['products']))\n",
    "        # EXCLUDE IF REACTION IN DATASET\n",
    "        # EXCLUDE TRAIN REACTION RULES\n",
    "        if (r not in reaction2unis) and (d['rule_id'] not in train_rules):\n",
    "            unannotated_reaction2ec[r].add(d['ec'])\n",
    "            unannotated_dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523fad2-aa40-4bb1-9553-90d8cd46e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unannotated_dataset), len(unannotated_reaction2ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613005c-939f-4f24-a3f7-8571a1d1889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([d['ec'] for d in unannotated_dataset]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81a59a-3247-4fa8-8024-82d5817c7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ECs per reaction: {Counter([len(v) for k,v in unannotated_reaction2ec.items() ])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "d44c4ab6-7e3e-428b-be50-a6dbe75519f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need ec2uniprots\n",
    "ec2uniprots = pickle.load(open('/home/Brenda/ec2uniprot_2023_1.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "9c1b425a-af7b-4b02-b552-1fa50161641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UniProt to EC classes, by level\n",
    "screenuni2ecs = {}\n",
    "for level in [1,2,3,4]:\n",
    "    s2ecs_ = defaultdict(set)\n",
    "    for ec, us in ec2uniprots.items():\n",
    "        for u in us:\n",
    "            s2ecs_[u].add(  '.'.join(ec.split('.')[:level])   )\n",
    "    screenuni2ecs[level] = s2ecs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e78713-0907-48a1-96a3-d79a0bc1854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(screenuni2ecs[level])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067561d-ff28-4369-ad53-aa3695a18fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c1e26-b248-44a0-a0eb-572928dfa7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unannotated samples\n",
    "data_samples = []\n",
    "\n",
    "for ref_id, ref_sample in tqdm(enumerate(unannotated_dataset), ncols=100, total=len(unannotated_dataset)):\n",
    "\n",
    "    rxn_uni = '.'.join(sorted(ref_sample['reactants'])) + '>>' + '.'.join(sorted(ref_sample['products']))\n",
    "\n",
    "    if rxn_uni in train_reactions:\n",
    "        continue \n",
    "\n",
    "    # check if reaction was already evaluated in this loop\n",
    "    if rxn_uni in reaction_set:\n",
    "        continue \n",
    "    \n",
    "    reaction_set.add(rxn_uni)\n",
    "    data_samples.append(ref_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4f290-788e-415e-8b6f-0c82c92b77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fcd85575-42fd-44d4-9164-aba97547ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process through rxn encoder\n",
    "batch_size = 5\n",
    "batches = [data_samples[i:(i+batch_size)] for i in range(0, len(data_samples), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b8ee6-2eeb-4149-ab0c-575a22b62510",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_substrate_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(batches, ncols=100, total=len(batches)):\n",
    "        bsamples = []\n",
    "        for ref_sample in batch:\n",
    "    \n",
    "            reactants = sorted([s for s in ref_sample['mapped_reactants'] if s != \"[H+]\"])\n",
    "            products = sorted([s for s in ref_sample['mapped_products'] if s != \"[H+]\"])\n",
    "            # products = [p for p in products if p not in reactants]\n",
    "            \n",
    "            reactants_graph, _ = from_mapped_smiles('.'.join(reactants), encode_no_edge=True)        \n",
    "            products_graph, _ = from_mapped_smiles('.'.join(products), encode_no_edge=True)\n",
    "    \n",
    "            if reactants_graph.x.shape != products_graph.x.shape:\n",
    "                z = 1/0\n",
    "            item = {\n",
    "                    \"reactants\": reactants_graph,\n",
    "                    \"products\": products_graph,\n",
    "                }\n",
    "            \n",
    "            bsamples.append(item)\n",
    "                \n",
    "        bsamples = loaders.default_collate(bsamples)\n",
    "        for key in ['reactants', 'products', 'graph', 'mol']:\n",
    "            if key in bsamples:\n",
    "                bsamples[key] = bsamples[key].to(DEVICE)\n",
    "        out = model.model(bsamples)\n",
    "        substrate_features = out['hidden']\n",
    "        substrate_features = substrate_features.cpu()\n",
    "        all_substrate_features.append(substrate_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18157040-b8c3-467e-b473-03a7b7505a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "287c801f-49a2-4061-bc9d-7747439b36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_substrate_features = torch.concat(all_substrate_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "521ab46d-1449-4b63-8628-04cb7e1a8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix between rxns and screening hiddens\n",
    "sim_matrix_ec = all_substrate_features @ screen_hiddens.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7bbc4e-205e-4698-94f9-b82499026823",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_ec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b691d-c42d-4c56-a513-fdaee3ea3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can loosen to higher levels\n",
    "level = 4\n",
    "\n",
    "percentiles = []\n",
    "metrics = defaultdict(list)\n",
    "reaction_set = set()\n",
    "\n",
    "for ref_id, ref_sample in tqdm(enumerate(data_samples), ncols=100, total=len(data_samples)):\n",
    "\n",
    "    # get reaction\n",
    "    rxn_uni = '.'.join(sorted(ref_sample['reactants'])) + '>>' + '.'.join(sorted(ref_sample['products']))\n",
    "\n",
    "    # if in train (shouldn't be) then skip\n",
    "    if rxn_uni in train_reactions:\n",
    "        continue \n",
    "\n",
    "    # check if reaction was already evaluated in this loop\n",
    "    if rxn_uni in reaction_set:\n",
    "        continue \n",
    "\n",
    "    # add so we skip next time\n",
    "    reaction_set.add(rxn_uni)\n",
    "\n",
    "    # get scores for this rxn\n",
    "    scores = sim_matrix_ec[ref_id].squeeze().numpy()\n",
    "\n",
    "    # screenuni2ecs\n",
    "    # ecs annotated for this reaction\n",
    "    ecs = set( '.'.join(e.split('.')[:level]) for e in unannotated_reaction2ec[rxn_uni] )\n",
    "    # label of: screening enzyme belongs to reaction ec\n",
    "    labels = [ bool(screenuni2ecs[level][u].intersection(ecs)) for u in screen_unis]\n",
    "\n",
    "    if not sum(labels):\n",
    "        continue\n",
    "    \n",
    "    # compute BEDROC        \n",
    "    labels, scores = np.array(labels), np.array(scores)\n",
    "\n",
    "    metrics[\"bedroc85\"].append( bedroc_score(labels, scores, decreasing=True, alpha=85.0) )\n",
    "    metrics[\"bedroc20\"].append( bedroc_score(labels, scores, decreasing=True, alpha=20.0) )\n",
    "    metrics[\"ef0.1\"].append( enrichment_score(labels, scores, decreasing=True, chi=0.1) )\n",
    "    metrics[\"ef0.05\"].append( enrichment_score(labels, scores, decreasing=True, chi=0.05) )\n",
    "\n",
    "    # percentile\n",
    "    # refscore = max(s for s,l in zip(scores, labels) if l)\n",
    "    # percentiles.append( percentileofscore(scores, refscore) )\n",
    "print_metrics(metrics, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33025acd-3efa-4a6d-be41-fcf8f5df8ace",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### terpenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "67b6e4b4-a1f3-4b07-a476-c9311f730499",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthases = json.load(open(\"/home/IOCB/TPS-rxn-carbon-mapped-curated-activity.json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3f461-e401-47cd-be17-c01b7ea1669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "terpene_reaction2unis = defaultdict(set)\n",
    "for ref_id, synthase_sample in tqdm(enumerate(synthases), total=len(synthases), ncols=100):\n",
    "\n",
    "    uni = synthase_sample['Uniprot ID']\n",
    "\n",
    "    if uni not in screen_unis_set:\n",
    "        continue\n",
    "        \n",
    "    reactants, _ = synthase_sample[\"rxn_smiles\"].split(\">>\")\n",
    "    reactants_= reactants.split(\".\")\n",
    "\n",
    "    edits = [ (int(x), int(y), t) for x,y,t in synthase_sample['bond_changes'] ]\n",
    "    rmol = Chem.MolFromSmiles(reactants)\n",
    "    products_ = robust_edit_mol(rmol, edits)\n",
    "    products = '.'.join(products_)\n",
    "    \n",
    "    std_reactants = sorted([remove_atom_maps(s) for s in reactants_])\n",
    "    std_products = sorted([remove_atom_maps(s) for s in products_])\n",
    "    rxn_uni = f\"{'.'.join(std_reactants)}>>{'.'.join(std_products)}\"\n",
    "    terpene_reaction2unis[rxn_uni].add(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "41f4a714-723f-454d-88b3-571109af4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "terpene_reaction2unis_train = {k:v for k,v in terpene_reaction2unis.items() if any(p in train_uniprots for p in v)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c30ec-a395-49d0-8047-0f040782f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(terpene_reaction2unis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3670eee-677d-4f9b-aee1-ad38f3408417",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(terpene_reaction2unis_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef60601-50b7-468c-b325-35380ae64c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = []\n",
    "metrics = defaultdict(list)\n",
    "data_samples = []\n",
    "data_sample_ids = []\n",
    "all_substrate_features = []\n",
    "reaction_set = set()\n",
    "synthes = set()\n",
    "\n",
    "screen_unis_set = set(screen_unis)\n",
    "with torch.no_grad():\n",
    "    for ref_id, synthase_sample in tqdm(enumerate(synthases), total=len(synthases), ncols=100):\n",
    "\n",
    "        uni = synthase_sample['Uniprot ID']\n",
    "\n",
    "        if uni not in screen_unis_set:\n",
    "            continue\n",
    "\n",
    "        # random 1 sample error\n",
    "        if \":999\" in synthase_sample[\"rxn_smiles\"]:\n",
    "            print(\"skipping 999\")\n",
    "            continue \n",
    "            \n",
    "        reactants, _ = synthase_sample[\"rxn_smiles\"].split(\">>\")\n",
    "        reactants_= reactants.split(\".\")\n",
    "\n",
    "        edits = [ (int(x), int(y), t) for x,y,t in synthase_sample['bond_changes'] ]\n",
    "        rmol = Chem.MolFromSmiles(reactants)\n",
    "        products_ = robust_edit_mol(rmol, edits)\n",
    "        products = '.'.join(products_)\n",
    "        pmol =  Chem.MolFromSmiles(products)\n",
    "        \n",
    "        std_reactants = sorted([remove_atom_maps(s) for s in reactants_])\n",
    "        std_products = sorted([remove_atom_maps(s) for s in products_])\n",
    "        rxn_uni = f\"{'.'.join(std_reactants)}>>{'.'.join(std_products)}\"\n",
    "\n",
    "        # IF DO NOT LOOK AT REACTIONS WITH UNI IN TRAI\n",
    "        if rxn_uni in terpene_reaction2unis_train:\n",
    "            continue\n",
    "\n",
    "        # check if reaction in train\n",
    "        if rxn_uni in train_reactions:\n",
    "            continue \n",
    "\n",
    "        # check in already evaluated\n",
    "        if rxn_uni in reaction_set:\n",
    "            continue \n",
    "\n",
    "        # check atom map is not overused incorrectly\n",
    "        atommapnumbers_r = set()\n",
    "        for atom in rmol.GetAtoms():\n",
    "            atommapnumbers_r.add(atom.GetProp(\"molAtomMapNumber\"))\n",
    "        atommapnumbers_p = set()\n",
    "        for atom in pmol.GetAtoms():\n",
    "            atommapnumbers_p.add(atom.GetProp(\"molAtomMapNumber\"))\n",
    "        if (len(atommapnumbers_p) != pmol.GetNumAtoms()) or (len(atommapnumbers_r) != rmol.GetNumAtoms()):\n",
    "            continue \n",
    "            \n",
    "        reactants_graph, _ = from_mapped_smiles(reactants, encode_no_edge=True)        \n",
    "        products_graph, _ = from_mapped_smiles(products, encode_no_edge=True)\n",
    "\n",
    "        if reactants_graph.x.shape != products_graph.x.shape:\n",
    "            continue\n",
    "\n",
    "        reaction_set.add(rxn_uni)\n",
    "        synthes.add(uni)\n",
    "        \n",
    "        data_samples.append( synthase_sample )\n",
    "        data_sample_ids.append(ref_id)\n",
    "\n",
    "        \n",
    "        item = {\n",
    "            \"reactants\": reactants_graph,\n",
    "            \"products\": products_graph,\n",
    "        }\n",
    "        \n",
    "        ref_sample = loaders.default_collate([item])\n",
    "        for key in ['reactants', 'products', 'graph', 'mol']:\n",
    "            if key in ref_sample:\n",
    "                ref_sample[key] = ref_sample[key].to(DEVICE)\n",
    "        out = model.model(ref_sample)\n",
    "        substrate_features = out['hidden']\n",
    "        substrate_features = substrate_features.cpu()\n",
    "        all_substrate_features.append(substrate_features)\n",
    "        \n",
    "        ref_unis = terpene_reaction2unis[rxn_uni]\n",
    "\n",
    "        scores = screen_hiddens @ substrate_features.T\n",
    "        scores = scores.squeeze().tolist()\n",
    "        \n",
    "        labels = [ int(u in ref_unis) for u in screen_unis]\n",
    "        \n",
    "        # compute metrics   \n",
    "        labels, scores = np.array(labels), np.array(scores)\n",
    "        metrics[\"bedroc85\"].append( bedroc_score(labels, scores, decreasing=True, alpha=85.0) )\n",
    "        metrics[\"bedroc20\"].append( bedroc_score(labels, scores, decreasing=True, alpha=20.0) )\n",
    "        metrics[\"ef0.1\"].append( enrichment_score(labels, scores, decreasing=True, chi=0.1) )\n",
    "        metrics[\"ef0.05\"].append( enrichment_score(labels, scores, decreasing=True, chi=0.05) )\n",
    "        \n",
    "        # percentile\n",
    "        refscore = max(s for s,l in zip(scores,labels) if l)\n",
    "        percentiles.append( percentileofscore(scores, refscore) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1b8d6-021a-4698-8473-af0031d6d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reaction_set), len(synthes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06bd68-19ee-4b1e-9b64-a7b6f6698e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics[\"ef0.05\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57900a-4e44-4fd9-b32b-6f189317a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(metrics, percentiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64e23f-35a9-428b-b2ae-dcd6c97202c7",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dac5ef-ba7a-4ad6-a23c-568dbe3e5fa3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Without train uniprots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f809d4-3dc0-423b-8041-24218372601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must remove reactions with train uni as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78908820-9762-4a65-bed3-b98ab8f0ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "nottrain_hiddens = torch.vstack([h for h,u in zip(screen_hiddens, screen_unis) if not (u in train_uniprots)])\n",
    "nottrain_uniprots = [u for u in screen_unis if not (u in train_uniprots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60c92911-4f58-489d-843b-9d66cf0d7d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = (all_substrate_features @ nottrain_hiddens.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103021f2-34a2-41b8-a69b-cc25e2292031",
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_set = set()\n",
    "percentiles = []\n",
    "metrics = defaultdict(list)\n",
    "\n",
    "for idx, ref_sample in tqdm(enumerate(data_samples), ncols=100, total=len(data_samples)):\n",
    "\n",
    "    if ref_sample['reaction_string'] in reaction_set:\n",
    "        continue\n",
    "    \n",
    "    labels = [ int(u in reaction2unis[ ref_sample['reaction_string'] ]) for u in nottrain_uniprots]\n",
    "    if not any(labels):\n",
    "        continue \n",
    "\n",
    "    reaction_set.add(ref_sample['reaction_string'])\n",
    "\n",
    "    scores = sim_matrix[idx].squeeze().tolist()\n",
    "    ref_score = max( scores[i] for i,l in enumerate(labels) if l)\n",
    "    \n",
    "    percentiles.append( percentileofscore(scores, ref_score ) )\n",
    "\n",
    "    metrics[\"bedroc85\"].append( bedroc_score(np.array(labels), np.array(scores), decreasing=True, alpha=85.0) )\n",
    "    metrics[\"bedroc20\"].append( bedroc_score(np.array(labels), np.array(scores), decreasing=True, alpha=20.0) )\n",
    "    metrics[\"ef0.1\"].append( enrichment_score(np.array(labels), np.array(scores), decreasing=True, chi=0.1) )\n",
    "    metrics[\"ef0.05\"].append( enrichment_score(np.array(labels), np.array(scores), decreasing=True, chi=0.05) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143fcfcc-fb63-45ac-8786-ef0ba3320fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(metrics, percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e6406e-7349-4491-a502-5ea67c9c2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metrics[\"bedroc85\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99dea8-eeeb-4060-b612-1302b982eace",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### by protein families: mmseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9e02cc70-ef0b-463b-aeae-aeb7938071c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmseqs\n",
    "mmseqs_clusters = pickle.load(open('/home/EnzymeMap/mmseq_clusters_updated.p', 'rb'))\n",
    "mmseq_cluster2uniprots = defaultdict(set)\n",
    "for k,v in mmseqs_clusters.items():\n",
    "    mmseq_cluster2uniprots[v].add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be341707-d883-4116-8e94-d2b789859529",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mmseqs_clusters), len(mmseq_cluster2uniprots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "865758aa-e696-4087-b6ff-31a0772b9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mmseqs_uniprots = set()\n",
    "for uni in train_uniprots:\n",
    "    train_mmseqs_uniprots.update( mmseq_cluster2uniprots[mmseqs_clusters[uni]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d898b2e5-72eb-4f61-ac6e-4fa7fa5f981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmseqs_hiddens = torch.vstack([h for h,u in zip(screen_hiddens, screen_unis) if not (u in train_mmseqs_uniprots)])\n",
    "mmseqs_all_ec_uniprots_ = [u for u in screen_unis if not (u in train_mmseqs_uniprots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796954a5-f5d3-4ed9-a686-bab56409d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = (all_substrate_features @ mmseqs_hiddens.T)\n",
    "\n",
    "mmseqs_samples = []\n",
    "percentiles = []\n",
    "reaction_set = set()\n",
    "metrics = defaultdict(list)\n",
    "\n",
    "for idx, ref_sample in tqdm(enumerate(data_samples), ncols=100, total=len(data_samples)):\n",
    "\n",
    "    if ref_sample['reaction_string'] in reaction_set:\n",
    "        continue\n",
    "    \n",
    "    if  train_dataset.ec2uniprot.get( ref_sample['ec'], None) is None:\n",
    "        continue\n",
    "\n",
    "    labels = [ int(u in reaction2unis[ ref_sample['reaction_string'] ]) for u in mmseqs_all_ec_uniprots_]\n",
    "    if not any(labels):\n",
    "        continue \n",
    "    \n",
    "    reaction_set.add(ref_sample['reaction_string'])\n",
    "\n",
    "    mmseqs_samples.append(ref_sample)\n",
    "    \n",
    "    scores = sim_matrix[idx].squeeze().tolist()\n",
    "\n",
    "    ref_score = max( scores[i] for i,l in enumerate(labels) if l)\n",
    "    \n",
    "    percentiles.append( percentileofscore(scores, ref_score ) )\n",
    "    \n",
    "    metrics[\"bedroc85\"].append( bedroc_score(np.array(labels), np.array(scores), decreasing=True, alpha=85.0) )\n",
    "    metrics[\"bedroc20\"].append( bedroc_score(np.array(labels), np.array(scores), decreasing=True, alpha=20.0) )\n",
    "    metrics[\"ef0.1\"].append( enrichment_score(np.array(labels), np.array(scores), decreasing=True, chi=0.1) )\n",
    "    metrics[\"ef0.05\"].append( enrichment_score(np.array(labels), np.array(scores), decreasing=True, chi=0.05) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bd25646d-2527-475a-bc0b-ffedf3748ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([ d['uniprot_id'] in train_mmseqs_uniprots for d in mmseqs_samples]), len(mmseqs_samples)\n",
    "label = [ d['uniprot_id'] in train_mmseqs_uniprots for d in mmseqs_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1988698-02c8-41bb-9f1c-0c247726693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(metrics, percentiles, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38d056-0752-46bd-ace0-050464507d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1811cc1d-2350-49a7-a864-ba2d241f5121",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### by protein families: foldseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "035ddf47-6a29-4251-b00b-37f0d6c656f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foldseek clusters\n",
    "foldseek_clusters = pickle.load(open('/home/EnzymeMap/foldseek/cov90_cluster.p', 'rb'))\n",
    "foldseek_cluster2uniprots = defaultdict(set)\n",
    "for k,v in foldseek_clusters.items():\n",
    "    foldseek_cluster2uniprots[v].add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b8a5b262-7b74-4751-ae4e-ec194f0bdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_foldseek_uniprots = set()\n",
    "for uni in train_uniprots:\n",
    "    train_foldseek_uniprots.update( foldseek_cluster2uniprots[ foldseek_clusters[uni]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a038564d-7d25-49c8-b806-88dd968ff997",
   "metadata": {},
   "outputs": [],
   "source": [
    "foldseek_hiddens = torch.vstack([h for h,u in zip(screen_hiddens, screen_unis) if not (u in train_foldseek_uniprots)])\n",
    "foldseek_all_ec_uniprots_ = [u for u in screen_unis if not (u in train_foldseek_uniprots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "efbc8339-36d5-4c85-aecb-e580682d2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = (all_substrate_features @ foldseek_hiddens.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f46d80-c5c8-4877-b1cb-f4eaf231758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = []\n",
    "data_samples_foldseek = []\n",
    "reaction_set = set()\n",
    "metrics = defaultdict(list)\n",
    "\n",
    "for idx, ref_sample in tqdm(enumerate(data_samples), ncols=100, total=len(data_samples)):\n",
    "\n",
    "    if ref_sample['reaction_string'] in reaction_set:\n",
    "        continue\n",
    "    \n",
    "    if  train_dataset.ec2uniprot.get( ref_sample['ec'], None) is None:\n",
    "        continue\n",
    "\n",
    "    labels = [ int(u in reaction2unis[ ref_sample['reaction_string'] ]) for u in foldseek_all_ec_uniprots_]\n",
    "    if not any(labels):\n",
    "        continue \n",
    "\n",
    "    reaction_set.add(ref_sample['reaction_string'])\n",
    "    data_samples_foldseek.append(ref_sample)\n",
    "    \n",
    "    scores = sim_matrix[idx].squeeze().tolist()\n",
    "\n",
    "    ref_score = max( scores[i] for i,l in enumerate(labels) if l)\n",
    "    \n",
    "    percentiles.append( percentileofscore(scores, ref_score ) )\n",
    "    \n",
    "    metrics[\"bedroc85\"].append( bedroc_score(np.array(labels), np.array(scores), decreasing=True, alpha=85.0) )\n",
    "    metrics[\"bedroc20\"].append( bedroc_score(np.array(labels), np.array(scores), decreasing=True, alpha=20.0) )\n",
    "    metrics[\"ef0.1\"].append( enrichment_score(np.array(labels), np.array(scores), decreasing=True, chi=0.1) )\n",
    "    metrics[\"ef0.05\"].append( enrichment_score(np.array(labels), np.array(scores), decreasing=True, chi=0.05) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c7684d1c-8da5-4610-8dd1-bf67184cd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([ d['uniprot_id'] in train_foldseek_uniprots for d in data_samples_foldseek]), len(data_samples_foldseek)\n",
    "label = [ d['uniprot_id'] in train_foldseek_uniprots for d in data_samples_foldseek]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210eb55d-0e2f-469f-8c5b-97328150ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(metrics, percentiles, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nox",
   "language": "python",
   "name": "nox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
